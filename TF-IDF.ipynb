{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from IPython.display import Image\n",
    "\n",
    "import nltk as nltk\n",
    "from nltk import tokenize\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize, sent_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import cmudict, stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from gensim import models, corpora, similarities\n",
    "import gensim as gensim\n",
    "from gensim.models import LsiModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.manifold import TSNE\n",
    "import sklearn as sklearn\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import math \n",
    "\n",
    "import textstat\n",
    "\n",
    "import textstat\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy\n",
    "\n",
    "import string\n",
    "import spacy \n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\drodill001\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\drodill001\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Error loading tokenize: Package 'tokenize' not found in\n",
      "[nltk_data]     index\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\drodill001\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\drodill001\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('tokenize')\n",
    "nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### IMPORTING FILE #########################\n",
    "def analyzer(report):\n",
    "        \n",
    "    def all(textsplit):\n",
    "        sentences = textsplit.split('.')\n",
    "        wordslist=textsplit.split(' ')\n",
    "\n",
    "        ##########################################################\n",
    "        ###################### PREPROCESSING #####################\n",
    "        ##########################################################\n",
    "\n",
    "        ############ TOKENIZATION ################################\n",
    "        ##########################################################\n",
    "\n",
    "        ############ words tokenized ##############################\n",
    "        words_lower = []\n",
    "        for i in wordslist:\n",
    "            tokens = i.lower()\n",
    "            words_lower.append(tokens)\n",
    "\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in words_lower]             \n",
    "        #stripped is a list with all tokenized, lower case and stripped words\n",
    "\n",
    "        ############ STOPWORD REMOVAL #############################\n",
    "        ###########################################################\n",
    "\n",
    "        ################## words without stopwords ################\n",
    "        wordslistnoalpha = [word for word in stripped if word.isalpha()]\n",
    "        relevant_words = [word for word in wordslistnoalpha if word not in stopwords.words('english')]\n",
    "        rel_words_sort = sorted(relevant_words)\n",
    "        rel_words_sort_nodup = list(dict.fromkeys(rel_words_sort)) \n",
    "        \n",
    "        ################## TF- IDF ##################################\n",
    "        ##############################################################\n",
    "        Term_occurences= []\n",
    "        for w in rel_words_sort_nodup:\n",
    "            Term_occurences.append(relevant_words.count(w))\n",
    "            Term_occurrences_array = numpy.array(Term_occurences)\n",
    "            Number_of_Words = len(relevant_words)\n",
    "            TF = Term_occurrences_array/Number_of_Words\n",
    "                \n",
    "        Word_TF = {}\n",
    "        for i in range(0,len(rel_words_sort_nodup)):\n",
    "            Word_TF[rel_words_sort_nodup[i]] = TF[i]\n",
    "        print(Word_TF)\n",
    "            \n",
    "    # Text processing read in entire file\n",
    "    text = ''\n",
    "    with open(report, \"r\") as file:\n",
    "        for line in file.readlines():\n",
    "            text += line\n",
    "\n",
    "    # Remove hard newlines and split text on ends of sentences\n",
    "    text = text.replace('\\n', '').replace('\\r', '')\n",
    "    # textsplit is a list of all sentences without '.'\n",
    "    chapter = text.split('SECTION')\n",
    "    print(\"Number of chapters in document:\", len(chapter))\n",
    "    \n",
    "    words_lower = []\n",
    "    for i in text:\n",
    "        tokens = i.lower()\n",
    "        words_lower.append(tokens)\n",
    "\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in words_lower]             \n",
    "\n",
    "    ############ STOPWORD REMOVAL #############################\n",
    "    ###########################################################\n",
    "\n",
    "    ################## words without stopwords ################\n",
    "    wordslistnoalpha = [word for word in stripped if word.isalpha()]\n",
    "    relevant_words = [word for word in wordslistnoalpha if word not in stopwords.words('english')]\n",
    "    rel_words_sort = sorted(relevant_words)\n",
    "    rel_words_sort_nodup = list(dict.fromkeys(rel_words_sort)) \n",
    "\n",
    "    ################### IDF ##################################\n",
    "    ##########################################################\n",
    "    count = 0\n",
    "    IDF = []\n",
    "    for w in rel_words_sort_nodup:\n",
    "        for i in chapter:\n",
    "            if w in i:\n",
    "                count += 1\n",
    "        count = math.log10(len(chapter)/count)  \n",
    "        IDF.append(count)\n",
    "    count=0\n",
    "    print(IDF)\n",
    "                \n",
    "#     Word_IDF = {}\n",
    "#     for i in range(0,len(rel_words_sort_nodup)):\n",
    "#         Word_IDF[rel_words_sort_nodup[i]] = IDF[i]\n",
    "#     print(WordIDF)\n",
    "\n",
    "\n",
    "#     #     #################### MAKING TF-IDF SCORE ##########################\n",
    "    #     TF_IDF = []\n",
    "\n",
    "    #     for i in Word_TF:\n",
    "    #         for j in Word_IDF:\n",
    "    #             score = 0\n",
    "    #             if i[0]==j[0]:\n",
    "    #                 score = i[1]*j[1]\n",
    "    #                 TF_IDF.append((i, score))\n",
    "\n",
    "    #         print(TF_IDF)\n",
    "\n",
    "#     for i in chapter:\n",
    "#         all(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chapters in document: 5\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09691001300805642, -0.008336961762198253, 0.0007247436819195821, -6.294587451807882e-05, 5.467443607703477e-06, -4.748958581094693e-07, 4.124893203684352e-08, -3.582836729333038e-09, 3.1120129890055255e-10, -2.703062926843644e-11, 2.3478480828109418e-12, 0.09691001300780144]\n"
     ]
    }
   ],
   "source": [
    "analyzer(r'C:\\Users\\drodill001\\Documents\\Google_annual_financial_report_2008._SECTIONStxt.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
